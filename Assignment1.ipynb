{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GitHub repository\n",
    "Link to repository used to colaborate on the assignment:\n",
    "https://github.com/KarolineKlan/Assignments_ComSocSci2024.git\n",
    "\n",
    "### Contribution statement\n",
    "\n",
    "Team members:\n",
    "\n",
    "- Jacob (s214596)\n",
    "- Kristoffer (s214609)\n",
    "- Karoline (s214638)\n",
    "\n",
    "All members collaborated and contributed to every part of the assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "This assignment was formed using Web-scraping tools from the program of the International Conference in Computational Social Science 2023  https://ic2s2-2023.org/program, and acessing data of Authors and Research Articles using the OpenAlex API https://docs.openalex.org/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "#Import relevant libraries\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from Levenshtein import distance\n",
    "import numpy as np\n",
    "import ast\n",
    "import networkx as nx\n",
    "from joblib import Parallel, delayed\n",
    "import os \n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Webscraping\n",
    "In the following task we use web-scraping tools to get the list of participants in the International Conference in Computational Social Science (CSC) 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique speakers:  1472\n",
      "Number of unique keynote speakers:  10\n",
      "Number of unique chairs:  49\n",
      "Number of unique members from optional link1:  333\n",
      "Number of unique teachers from optional link2:  19\n",
      "Total number of unique speakers:  1645\n"
     ]
    }
   ],
   "source": [
    "# define link to scrape, and beautifulsoup object\n",
    "LINK = \"https://ic2s2-2023.org/program\"\n",
    "LINK_OPTIONAL1 = \"https://ic2s2-2023.org/program_committee\"\n",
    "r_OPTIONAL1 = requests.get(LINK_OPTIONAL1)\n",
    "soup_OPTIONAL1 = BeautifulSoup(r_OPTIONAL1.content)\n",
    "LINK_OPTIONAL2 = \"https://ic2s2-2023.org/tutorials\"\n",
    "r_OPTIONAL2 = requests.get(LINK_OPTIONAL2)\n",
    "soup_OPTIONAL2 = BeautifulSoup(r_OPTIONAL2.content)\n",
    "r = requests.get(LINK)\n",
    "soup = BeautifulSoup(r.content)\n",
    "\n",
    "# Find all relevant places in the HTML code where names are stored\n",
    "speaker = soup.findAll(\"ul\", {\"class\" : \"nav_list\"})\n",
    "chair = soup.findAll(\"h2\")\n",
    "table = soup.find(\"table\", {\"class\" : \"tutorials\"})\n",
    "table = table.find_all(\"td\")\n",
    "main = soup_OPTIONAL1.find(\"section\", {\"id\" : \"main\"})\n",
    "names_members = main.findAll(\"li\")\n",
    "names_teachers = soup_OPTIONAL2.findAll(\"div\", {\"class\" : \"col-5 col-12-medium\"})\n",
    "\n",
    "# Loop through the HTML code and extract names\n",
    "keynote_names = [table[k].text.lower().split(\"- \")[1] for k in range(len(table)) if \"Keynote\" in table[k].text]\n",
    "chair_names = [chair[k].text.lower().split(\": \")[2] for k in range(len(chair)) if \"Chair\" in chair[k].text]\n",
    "speaker_names = [speaker[k].find_all(\"i\")[j].text.lower().split(\", \")  for k in range(len(speaker)) for j in range(len(speaker[k].find_all(\"i\")))]\n",
    "speaker_names = sum(speaker_names, [])\n",
    "names_members_lst = [names_members[i].find(\"b\").text.lower() for i in range(len(names_members))]\n",
    "names_teachers = [names_teachers[i].findAll(\"li\")[k].find(\"b\").text.lower() for i in range(len(names_teachers)) for k in range(len(names_teachers[i].findAll(\"li\")))]\n",
    "\n",
    "\n",
    "\n",
    "# Print results for each category\n",
    "print(f\"Number of unique speakers:  {len(set(speaker_names))}\")\n",
    "print(f\"Number of unique keynote speakers:  {len(set(keynote_names))}\")\n",
    "print(f\"Number of unique chairs:  {len(set(chair_names))}\")\n",
    "print(f\"Number of unique members from optional link1:  {len(set(names_members_lst))}\")\n",
    "print(f\"Number of unique teachers from optional link2:  {len(set(names_teachers))}\")\n",
    "\n",
    "# Add all names together to find total unique names\n",
    "total_names = speaker_names + keynote_names + chair_names + names_members_lst + names_teachers\n",
    "df = pd.DataFrame(total_names, columns = [\"Name\"])\n",
    "df[\"Name\"] = df[\"Name\"].str.replace(\".\", \"\")\n",
    "df[\"Name\"] = df[\"Name\"].str.lstrip(\" \")\n",
    "uniq_names = pd.DataFrame(set(df[\"Name\"]), columns=[\"Name\"])\n",
    "uniq_names = uniq_names.sort_values('Name', ascending=True)\n",
    "\n",
    "print(f\"Total number of unique speakers:  {len((uniq_names))}\")\n",
    "\n",
    "pd.DataFrame(uniq_names).to_csv(\"data/authors_part1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**The process of the web-scraping:** \n",
    "\n",
    "In the process of web-scraping the website and collect the specific names of all the researchers, a thourough investigation of the HTML setup was initiated in order to understand the hierarchical and nested structure of the page. The main structure of the page was after inspection divided into 3 main parts, where different approaches were utilized in order to access the data from different structures:\n",
    "1. collect the names og the key-note speakers from the overview table structure\n",
    "2. collect the names for the chair speakers in the \"h2\" sections\n",
    "3. collect names from the text sections in the \"ul\" sections \n",
    "\n",
    "And later the optional parts was added extracting the members using find_all on the \"li\" structure, and teachers in the special class \"col-5 col-12-medium\". \n",
    "\n",
    "When collecting all the names, the strings were all converted to lower-case and dots, initial space and other characters were stripped from the strings in order to mitigate having the same person spelled with specific middlename breveations. The set() function makes sure we only save unique names.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Ready Made vs Custom Made Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Custom-made data**, as used in Centola's experiment, offers the advantage of experimental control and manipulation, allowing the researchers to precisely design conditions and variables and data is not incomplete, since all the needed data is collected. However, it can be time-consuming and costly to collect, and it may lack real-world validity due to potential response biases when trying to record behaviors with digital systems that are highly engineered to induce specific behaviors. \n",
    "**Ready-made data**, like used in Nicolaides's study, offers readily available information as it is always-on, and then enables the study of unexpected events and real-time measurements. It reflects real-world behavior but may lack experimental control and relevance to specific research questions, as samples can be incomplete or biased. It also poses challenges in identifying and controlling confounding variables and can be dirty and difficult to access.\n",
    "\n",
    "2. Considering the results gathered from Centola's custom-made data, it allows for clear causal inference regarding the spread of behavior, but the experiment is made within an artificial environment, potentially limiting the applicability of the results to real-life settings and the overall generalizability of the experiment. In the interpretation of the results from Nicolaides's ready-made data confounding variables and biases that might have been involved could be hard to identify, which could complicate the interpretation of the results and potentially lead to wrong conclusions. Additionally, bias within both types of data could affect the results, which is important to consider in the interpretation. In custom-made data response biases should be considered, as researchers might induce specific types of behaviors based on the design of the experiment. In ready-made data bias induced by potentially incomplete or nonrepresentative datasets, and other characteristics of big data could affect the results and should also be considered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Gathering Research Articles using the OpenAlex API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the assignment the API endpoint \"works\" from the OpenAlex API is used to collect Research Articles from IC2S2 Authors.\n",
    "\n",
    "A dataframe called \"authors_part3.csv\" from week 2 exercise 2 is loaded in order to retrieve the works from the authors. In week 2 the author_info of the authors retrieved in part 1 is reduced from being the length of 1645 to 1257, because the Levenshtein distance measure is utilized to make sure the correct names are retrieved from the openAlex API. We exclude names where the Levenshtein distance is above 6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'https://api.openalex.org/'\n",
    "RESOURCE = 'works'\n",
    "COMPLETE_URL = BASE_URL + RESOURCE\n",
    "\n",
    "#load df from week 2 exercise 2\n",
    "df = pd.read_csv('data/authors_part3.csv', index_col=[0])\n",
    "\n",
    "#filter out authors with less than 5 works and more than 5000 works\n",
    "IC2S2_author = df.loc[df['works_count'] > 5]\n",
    "IC2S2_authors = IC2S2_author.loc[IC2S2_author['works_count'] < 5000]\n",
    "IC2S2_authors = IC2S2_authors.reset_index()\n",
    "\n",
    "#split the list of authors into chunks of 25\n",
    "lst = [str(IC2S2_authors['id'][i]) + '|' for i in range(len(IC2S2_authors['id']))]\n",
    "n = 25\n",
    "chunked_lists = [lst[i:i + n] for i in range(0, len(lst), n)]\n",
    "\n",
    "# Specify the Filters used in the filtering process for the API\n",
    "Filter1 = \"cited_by_count:>10\"\n",
    "Filter2 = \"authors_count:<10\"\n",
    "\n",
    "all_concepts = requests.get(BASE_URL+\"concepts\", params={\"filter\":\"level:0\"}).json()\n",
    "\n",
    "social_concepts= [i[\"id\"] for i in all_concepts[\"results\"] if i['display_name'] in ([\"Sociology\",\"Psychology\",\"Economics\",\"Political Science\"])]\n",
    "math_concepts= [i[\"id\"] for i in all_concepts[\"results\"] if i['display_name'] in ([\"Mathematics\",\"Physics\",\"Computer Science\"])]\n",
    "\n",
    "Filter3=\"concepts.id:\"+\"|\".join(social_concepts)\n",
    "Filter4=\"concepts.id:\"+\"|\".join(math_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/42 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:31<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "IC2S2_papers = {}\n",
    "IC2S2_abstracts = {}\n",
    "\n",
    "# Define function to get papers and abstracts from the API in chunks of 25 authors\n",
    "def get_papers_and_abstracts(chunked_list):\n",
    "    ids = ''.join(chunked_list)\n",
    "    ids = ids[:-1]\n",
    "    cursorState = '*'\n",
    "    while cursorState:\n",
    "        PARAMS = {\"per_page\":200,\n",
    "                \"filter\" :f'{\",\".join([Filter1,Filter2,Filter3,Filter4])},authorships.author.id:{ids}',\n",
    "                \"cursor\": f'{cursorState}'}\n",
    "            \n",
    "        response = requests.get(COMPLETE_URL, params=PARAMS).json()\n",
    "        if response['meta']['next_cursor']:\n",
    "            cursorState = str(response['meta']['next_cursor'])\n",
    "        else: cursorState = None\n",
    "\n",
    "        for i in range(len(response['results'])):\n",
    "            id = response['results'][i]['id']\n",
    "            pub_year = response['results'][i]['publication_year']\n",
    "            cited = response['results'][i]['cited_by_count']\n",
    "            authors = [response['results'][i]['authorships'][l]['author']['id'] for l in range(len(response['results'][i]['authorships']))]\n",
    "            title = response['results'][i]['title']\n",
    "            abstract = response['results'][i]['abstract_inverted_index']\n",
    "            IC2S2_papers[str(id)] = {'id':id, 'publication_year':pub_year,'cited_by_count':cited, 'author_ids':authors}\n",
    "            IC2S2_abstracts[str(id)] = {'id':id, 'title':title, 'abstract_inverted_index':abstract}\n",
    "    \n",
    "    return IC2S2_papers, IC2S2_abstracts\n",
    "\n",
    "# Use joblib to parallelize the process of getting papers and abstracts faster\n",
    "results = Parallel(n_jobs=2)(delayed(get_papers_and_abstracts)(chunked_list) for chunked_list in tqdm(chunked_lists))\n",
    "\n",
    "\n",
    "papers_dict = {k: v for result in results for k, v in result[0].items()}\n",
    "abstracts_dict = {k: v for result in results for k, v in result[1].items()}\n",
    "\n",
    "Papers = pd.DataFrame(papers_dict).transpose()\n",
    "Abstracts = pd.DataFrame(abstracts_dict).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of works in the IC2S2_papers dataframe: 4490\n",
      "Number of unique authors that have co-authored the works: 8488\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of works in the IC2S2_papers dataframe: {len(Papers)}\")\n",
    "print(f\"Number of unique authors that have co-authored the works: {len(Papers['author_ids'].explode().unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Overview and Reflection questions:**\n",
    "\n",
    "##### **How many works are listed in your IC2S2 papers dataframe?**\n",
    "\n",
    "- 4490 works are listed in the IC2S2 papers dataframe. \n",
    "\n",
    "##### **How many unique researchers have co-authored these works?**\n",
    "\n",
    "- The number of unique researchers is 8488.\n",
    "\n",
    "##### **Efficiency in code**\n",
    "- In the approach of making the code more efficient two main things was considered. It was realized that the bottleneck in the code was the rate limit of the API which is 2 request per second. Therefore it was implemented, instead of using the 'search' parameter where one author could be requested at a time,  to use in the filter parameter 'authorships.author.id' to request 25 authors at a time where we concatenated the author ids with the \"|\" (OR) parameter. In this way it was managed to request 25 authors at a time.\n",
    "Furthermore, the python library 'joblib' was used to parallelize the requests to the API. It was decided to change all the for loops in the code, to functions and run it through joblib to run more jobs at a time. This really had an impact in the run-time of the code, and made it more efficient.\n",
    "\n",
    "\n",
    "##### **Filtering Criteria and Dataset Relevance**\n",
    "- By setting the filter on the API request to a threshold of works count between 5 and 5000, ensures that only authors with some amount of work was represented in the dataset. To further ensure that the dataset is relevant, a filter was set to the citation count to be above 5 and that the amount of authors on the work is below 10. This ensures that the works and authors are relevant since the works should have been cited at least 5 times and too many authors on one work makes every author less impactfull on the specific work. Furthermore a filter was set on the works that included both either \"Sociology\", \"Psychology\", \"Economics\", \"Political Science\" or \"Mathematics\",\"Physics\",\"Computer Science\". This leads to an overrepresentation in these fields and will lead to an underrepresentation in fields that include \"History\", \"Biology\", \"Medicine\" and more fields which could also have been relevant to this study, which is important to keep in mind.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Network of Computational Social Scientists\n",
    "\n",
    "In this part of the assignment we construct and investigate the Computational Social Scientists Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1 Weighted edgelist made from Papers dataset\n",
    "def find_pairs(my_list): # A function to rearange a list into a list of tuples of pairs\n",
    "    pairs = []\n",
    "    for n, author in enumerate(my_list): \n",
    "        for author2 in my_list[n+1:]:\n",
    "            pairs.append((author,author2))\n",
    "    return pairs\n",
    "\n",
    "pairs = Papers['author_ids'].apply(lambda x: tuple(sorted(x))).apply(find_pairs)        # rearange id's to avoid duplicates eg. (a,b),(b,a)\n",
    "all_pairs = pairs.explode()\n",
    "sorted_pairs = all_pairs.groupby(all_pairs).count().sort_values()                       # Sort all pairs of authors by the amount of works done together\n",
    "edgelist = [(a1,a2,v) for (a1,a2), v in zip(sorted_pairs.index, sorted_pairs.values)]   # create a list of tuples with the two authors and the amount of combined works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Graph construction based on the created edgelist\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(edgelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 save authors as nodes in the graph, with certain information about each, saved as attributes.\n",
    "# Note that we employ the dataset of all found authors from week 3 exercise 2, in order to aquire the relevant information for all authors in the graph.\n",
    "Author_df = pd.read_csv('data/IC2S2_all_authors.csv')\n",
    "\n",
    "Long_paper_df = Papers.explode('author_ids')\n",
    "\n",
    "# Find the smallest publication year for each author\n",
    "publication_df = Long_paper_df.groupby('author_ids')['publication_year'].min().reset_index()\n",
    "publication_df.columns = ['id', 'first_publication_year']\n",
    "\n",
    "# Find the number of papers each author has been cited\n",
    "cited_df = Long_paper_df.groupby('author_ids')['cited_by_count'].sum().reset_index()\n",
    "cited_df.columns = ['id','cited_by_count']\n",
    "\n",
    "# Add the information to the Author_df \n",
    "Author_df = Author_df.merge(publication_df, on='id')\n",
    "Author_df = Author_df.merge(cited_df, on='id')\n",
    "\n",
    "# Remove eventual Nan values\n",
    "Author_df = Author_df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Add the information to each node in the graph \n",
    "for node in G.nodes:\n",
    "    if node in Author_df['id'].values:\n",
    "        G.nodes[node]['display_name'] = Author_df[Author_df['id']== node]['display_name'].values[0]\n",
    "        G.nodes[node]['country_code'] = Author_df[Author_df['id']== node]['country_code'].values[0]\n",
    "        G.nodes[node]['cited_by_count'] = Author_df[Author_df['id']== node]['cited_by_count'].values[0]\n",
    "        G.nodes[node]['first_publication_year'] = Author_df[Author_df['id']== node]['first_publication_year'].values[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Save the graph as a JSON file\n",
    "\n",
    "# Convert the graph to a dictionary\n",
    "graph_dict = nx.node_link_data(G)\n",
    "\n",
    "# Convert numpy.int64 to int for JSON serialization\n",
    "def convert(o):\n",
    "    if isinstance(o, np.int64):\n",
    "        return int(o)\n",
    "    raise TypeError\n",
    "\n",
    "# Write the graph dictionary to a JSON file\n",
    "with open(\"data/network_with_attributes.json\", \"w\") as f:\n",
    "    json.dump(graph_dict, f, default=convert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2 of part 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of nodes in G is 8478 and the amount of edges are 23778\n",
      "\n",
      "The denisty of G is 0.0006617130855140186\n",
      "\n",
      "Is the graph fully connected (not disconnected): False\n",
      "\n",
      "Since graph G is disconnected; there are 261 different connected components and 0 isolated nodes\n"
     ]
    }
   ],
   "source": [
    "#2.1 Network metrics\n",
    "CC = 0\n",
    "IN = 0\n",
    "components = nx.connected_components(G)\n",
    "for c in components:\n",
    "    CC += 1\n",
    "    if len(c) == 1:\n",
    "        IN += 1\n",
    "\n",
    "print(f'The number of nodes in G is {G.number_of_nodes()} and the amount of edges are {G.number_of_edges()}')\n",
    "print(f'\\nThe denisty of G is {nx.density(G)}')\n",
    "print(f'\\nIs the graph fully connected (not disconnected): {nx.is_connected(G)}')\n",
    "print(f'\\nSince graph G is disconnected; there are {CC} different connected components and {IN} isolated nodes')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Network Metrics**\n",
    "\n",
    "The low network density compared to a fully connected network is in line with our expectations since a fully connected network would suggest that every scientist should have connections with all other scientists in the field. The density multiplied with the amount of nodes (Authors) suggest that on average, the number of connections between authors is: den x nodes ≈ 6.\n",
    " There are alot of disconnected components, which might suggest some separation within the field of social science, this could be due to different barriers such as geographical placement, language, specific research area etc. This is also within what we would expect of a network spanning across such a broad research field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regarding the degree of nodes in G. \n",
      "average : 5.6093418259023355\n",
      "median : 4\n",
      "mode 3\n",
      "min : 1\n",
      "max : 122 \n",
      "\n",
      "Regarding the weighted degree of nodes in G. \n",
      "average : 6.864354800660533\n",
      "median : 5\n",
      "mode 3\n",
      "min : 1\n",
      "max : 212 \n"
     ]
    }
   ],
   "source": [
    "s1 = sorted([val for idx, val in G.degree])\n",
    "s2 = sorted([val for idx, val in G.degree(weight='weight')])\n",
    "for n,sorted_degrees in enumerate([s1, s2]):\n",
    "    average_degree = sum(sorted_degrees)/G.number_of_nodes()\n",
    "    median_degree = sorted_degrees[int(len(sorted_degrees)/2)]\n",
    "    mode_degree = max(set(sorted_degrees),key=sorted_degrees.count)\n",
    "    min_degree = sorted_degrees[0]\n",
    "    max_degree = sorted_degrees[-1]\n",
    "    if n == 0:\n",
    "        print(f'Regarding the degree of nodes in G. \\naverage : {average_degree}\\nmedian : {median_degree}\\nmode {mode_degree}\\nmin : {min_degree}\\nmax : {max_degree} \\n')\n",
    "    else: print(f'Regarding the weighted degree of nodes in G. \\naverage : {average_degree}\\nmedian : {median_degree}\\nmode {mode_degree}\\nmin : {min_degree}\\nmax : {max_degree} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Degree analysis**\n",
    "\n",
    "The metrics on degrees tell us how many other scientists any given scientist have collaborated with, such as what the average amount of connections any given scientist has, which is between 5 -6 and what the most usual (mode) amount of connections is, which is 3. The analysis of weighted degrees on the other hand tell us about the amount of works any given scientist have worked on together with another author from our dataset. Here the metrics in general are slightly higher, indicating that scientists often produce more than one work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>degree</th>\n",
       "      <th>names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2376</th>\n",
       "      <td>https://openalex.org/A5088141761</td>\n",
       "      <td>122</td>\n",
       "      <td>Jonathan D. Cohen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>https://openalex.org/A5029100305</td>\n",
       "      <td>104</td>\n",
       "      <td>Denny Borsboom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>https://openalex.org/A5075080019</td>\n",
       "      <td>95</td>\n",
       "      <td>Qin Li</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>https://openalex.org/A5055710645</td>\n",
       "      <td>94</td>\n",
       "      <td>Jon Kleinberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>https://openalex.org/A5065243448</td>\n",
       "      <td>92</td>\n",
       "      <td>Qin Wang</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id  degree              names\n",
       "2376  https://openalex.org/A5088141761     122  Jonathan D. Cohen\n",
       "2746  https://openalex.org/A5029100305     104     Denny Borsboom\n",
       "103   https://openalex.org/A5075080019      95             Qin Li\n",
       "1586  https://openalex.org/A5055710645      94      Jon Kleinberg\n",
       "533   https://openalex.org/A5065243448      92           Qin Wang"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.3 Top 5 Authors, based on the amount of degrees (collaborative works with unique authors)\n",
    "top5 = pd.DataFrame(G.degree, columns=['id', 'degree']).sort_values(by='degree',ascending=False).head(5)\n",
    "top5['names'] = None\n",
    "display_names = [Author_df[Author_df['id']==node]['display_name'].values[0] for node in top5['id']]\n",
    "top5['names'] = display_names\n",
    "top5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 Authors\n",
    "\n",
    "Jonathan D. Cohen specializes in cognitive neuroscience, studying brain mechanisms influencing human behavior. Qin Li's work is concerned with genetics and developmental biology. Denny Borsboom's research focuses on psychology and applying network theory to study mental disorders. Jon Kleinberg is a computer scientist, focusing on algorithms and networks, while Qin Wang is a chemist. While all these researchers work in scientific domains, not all of them explicitly align with the themes of Computational Social Science. Cohen, Borsboom, and Kleinberg's work have elements of CSS, as they work with human behavior, network theory in psychology, and algorithms. However, Qin Li and Wang, as a biologist and chemist, work in areas that could be seen as not as relevant regarding CSS. A reason that these scientist ends in the top 5 anyways might be because they made a work very popular within CSS but their primary work migh not specifically revolve around CSS studies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
