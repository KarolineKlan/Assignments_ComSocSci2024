{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GitHub repository\n",
    "Link to repository used to colaborate on the assignment:\n",
    "https://github.com/KarolineKlan/Assignments_ComSocSci2024.git\n",
    "\n",
    "### Contribution statement\n",
    "\n",
    "Team members:\n",
    "\n",
    "- Jacob (s214596)\n",
    "- Kristoffer (s214609)\n",
    "- Karoline (s214638)\n",
    "\n",
    "All members collaborated and contributed to every part of the assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "This assignment was formed using Web-scraping tools from the program of the International Conference in Computational Social Science 2023  https://ic2s2-2023.org/program, and acessing data of Authors and Research Articles using the OpenAlex API https://docs.openalex.org/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import ast\n",
    "import networkx as nx\n",
    "from "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Webscraping\n",
    "In the following task we use web-scraping tools to get the list of participants in the International Conference in Computational Social Science (CSC) 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique speakers:  1472\n",
      "Number of unique keynote speakers:  10\n",
      "Number of unique chairs:  49\n",
      "Number of unique members from optional link1:  333\n",
      "Number of unique teachers from optional link2:  19\n",
      "Total number of unique speakers:  1645\n"
     ]
    }
   ],
   "source": [
    "# define link to scrape, and beautifulsoup object\n",
    "LINK = \"https://ic2s2-2023.org/program\"\n",
    "LINK_OPTIONAL1 = \"https://ic2s2-2023.org/program_committee\"\n",
    "r_OPTIONAL1 = requests.get(LINK_OPTIONAL1)\n",
    "soup_OPTIONAL1 = BeautifulSoup(r_OPTIONAL1.content)\n",
    "LINK_OPTIONAL2 = \"https://ic2s2-2023.org/tutorials\"\n",
    "r_OPTIONAL2 = requests.get(LINK_OPTIONAL2)\n",
    "soup_OPTIONAL2 = BeautifulSoup(r_OPTIONAL2.content)\n",
    "r = requests.get(LINK)\n",
    "soup = BeautifulSoup(r.content)\n",
    "\n",
    "# Find all relevant places in the HTML code where names are stored\n",
    "speaker = soup.findAll(\"ul\", {\"class\" : \"nav_list\"})\n",
    "chair = soup.findAll(\"h2\")\n",
    "table = soup.find(\"table\", {\"class\" : \"tutorials\"})\n",
    "table = table.find_all(\"td\")\n",
    "main = soup_OPTIONAL1.find(\"section\", {\"id\" : \"main\"})\n",
    "names_members = main.findAll(\"li\")\n",
    "names_teachers = soup_OPTIONAL2.findAll(\"div\", {\"class\" : \"col-5 col-12-medium\"})\n",
    "\n",
    "# Loop through the HTML code and extract names\n",
    "keynote_names = [table[k].text.lower().split(\"- \")[1] for k in range(len(table)) if \"Keynote\" in table[k].text]\n",
    "chair_names = [chair[k].text.lower().split(\": \")[2] for k in range(len(chair)) if \"Chair\" in chair[k].text]\n",
    "speaker_names = [speaker[k].find_all(\"i\")[j].text.lower().split(\", \")  for k in range(len(speaker)) for j in range(len(speaker[k].find_all(\"i\")))]\n",
    "speaker_names = sum(speaker_names, [])\n",
    "names_members_lst = [names_members[i].find(\"b\").text.lower() for i in range(len(names_members))]\n",
    "names_teachers = [names_teachers[i].findAll(\"li\")[k].find(\"b\").text.lower() for i in range(len(names_teachers)) for k in range(len(names_teachers[i].findAll(\"li\")))]\n",
    "\n",
    "\n",
    "\n",
    "# Print results for each category\n",
    "print(f\"Number of unique speakers:  {len(set(speaker_names))}\")\n",
    "print(f\"Number of unique keynote speakers:  {len(set(keynote_names))}\")\n",
    "print(f\"Number of unique chairs:  {len(set(chair_names))}\")\n",
    "print(f\"Number of unique members from optional link1:  {len(set(names_members_lst))}\")\n",
    "print(f\"Number of unique teachers from optional link2:  {len(set(names_teachers))}\")\n",
    "\n",
    "# Add all names together to find total unique names\n",
    "total_names = speaker_names + keynote_names + chair_names + names_members_lst + names_teachers\n",
    "df = pd.DataFrame(total_names, columns = [\"Name\"])\n",
    "df[\"Name\"] = df[\"Name\"].str.replace(\".\", \"\")\n",
    "uniq_names = pd.DataFrame(set(df[\"Name\"]), columns=[\"Name\"])\n",
    "uniq_names[\"Name\"] = uniq_names[\"Name\"].str.lstrip(\" \")\n",
    "uniq_names = uniq_names.sort_values('Name', ascending=True)\n",
    "print(f\"Total number of unique speakers:  {len((uniq_names))}\")\n",
    "\n",
    "pd.DataFrame(uniq_names).to_csv(\"data/authors_part1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**The process of the web-scraping:** \n",
    "\n",
    "In the process of web-scraping the website and collect the specific names of all the researchers, a thourough investigation of the HTML setup was initiated in order to understand the hierarchical and nested structure of the page. The main structure of the page was after inspection divided into 3 main parts, where different approaches were utilized in order to access the data from different structures:\n",
    "1. collect the names og the key-note speakers from the overview table structure\n",
    "2. collect the names for the chair speakers in the \"h2\" sections\n",
    "3. collect names from the text sections in the \"ul\" sections "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Ready Made vs Custom Made Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Gathering Research Articles using the OpenAlex API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1645 [00:00<?, ?it/s]1154.83s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "1154.83s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "100%|██████████| 1645/1645 [07:55<00:00,  3.46it/s]\n"
     ]
    }
   ],
   "source": [
    "import requests # import requests module\n",
    "from tqdm import tqdm # import tqdm module\n",
    "import pandas as pd\n",
    "from Levenshtein import distance\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "authors = pd.read_csv(\"data/authors_part1.csv\")\n",
    "authors = authors[\"Name\"].tolist()\n",
    "\n",
    "BASE_URL = \"https://api.openalex.org/\"\n",
    "RESOURCE = \"authors\"\n",
    "\n",
    "complete_url = BASE_URL + RESOURCE\n",
    "storage = []\n",
    "\n",
    "def get_author_week2(names):\n",
    "    \n",
    "    PARAMETERS = {\"search\" : names}\n",
    "    response = requests.get(complete_url, params=PARAMETERS).json()\n",
    "    \n",
    "    if response[\"meta\"][\"count\"] > 0:\n",
    "        if distance(names, response[\"results\"][0][\"display_name\"]) < 6:\n",
    "            id = response[\"results\"][0][\"id\"]\n",
    "            display_name = response[\"results\"][0][\"display_name\"]\n",
    "            works_api_url = response[\"results\"][0][\"works_api_url\"]\n",
    "            h_index = response[\"results\"][0][\"summary_stats\"][\"h_index\"]\n",
    "            works_count = response[\"results\"][0][\"works_count\"]\n",
    "            \n",
    "            if response[\"results\"][0][\"last_known_institution\"] != None:\n",
    "                country_code = response[\"results\"][0][\"last_known_institution\"][\"country_code\"]\n",
    "            else:\n",
    "                country_code = \"No last known institution\"\n",
    "            storage.append([id, display_name, works_api_url, h_index, works_count, country_code])\n",
    "            \n",
    "    return storage\n",
    "\n",
    "\n",
    "\n",
    "df = Parallel(n_jobs=2)(delayed(get_author_week2)(names) for names in tqdm(authors))\n",
    "\n",
    "\n",
    "df = pd.DataFrame(df, columns=[\"id\", \"display_name\", \"works_api_url\", \"h_index\", \"works_count\", \"country_code\"])\n",
    "df = df.drop_duplicates(subset=\"id\", keep='first')\n",
    "df.to_csv(\"data/authors_part3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Overview and Reflection questions: \n",
    "\n",
    "- **Dataset summary.** \n",
    "\n",
    "- **Efficiency in code.** \n",
    "\n",
    "\n",
    "- **Filtering Criteria and Dataset Relevance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Network of Computational Social Scientists\n",
    "\n",
    "In this part of the assignment we construct and investigate the Computational Social Scientists Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comsocsci2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
