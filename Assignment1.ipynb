{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GitHub repository\n",
    "Link to repository used to colaborate on the assignment:\n",
    "https://github.com/KarolineKlan/Assignments_ComSocSci2024.git\n",
    "\n",
    "### Contribution statement\n",
    "\n",
    "Team members:\n",
    "\n",
    "- Jacob (s214596)\n",
    "- Kristoffer (s214609)\n",
    "- Karoline (s214638)\n",
    "\n",
    "All members collaborated and contributed to every part of the assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "This assignment was formed using Web-scraping tools from the program of the International Conference in Computational Social Science 2023  https://ic2s2-2023.org/program, and acessing data of Authors and Research Articles using the OpenAlex API https://docs.openalex.org/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from Levenshtein import distance\n",
    "import numpy as np\n",
    "import ast\n",
    "import networkx as nx\n",
    "from joblib import Parallel, delayed\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Webscraping\n",
    "In the following task we use web-scraping tools to get the list of participants in the International Conference in Computational Social Science (CSC) 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique speakers:  1472\n",
      "Number of unique keynote speakers:  10\n",
      "Number of unique chairs:  49\n",
      "Number of unique members from optional link1:  333\n",
      "Number of unique teachers from optional link2:  19\n",
      "Total number of unique speakers:  1645\n"
     ]
    }
   ],
   "source": [
    "# define link to scrape, and beautifulsoup object\n",
    "LINK = \"https://ic2s2-2023.org/program\"\n",
    "LINK_OPTIONAL1 = \"https://ic2s2-2023.org/program_committee\"\n",
    "r_OPTIONAL1 = requests.get(LINK_OPTIONAL1)\n",
    "soup_OPTIONAL1 = BeautifulSoup(r_OPTIONAL1.content)\n",
    "LINK_OPTIONAL2 = \"https://ic2s2-2023.org/tutorials\"\n",
    "r_OPTIONAL2 = requests.get(LINK_OPTIONAL2)\n",
    "soup_OPTIONAL2 = BeautifulSoup(r_OPTIONAL2.content)\n",
    "r = requests.get(LINK)\n",
    "soup = BeautifulSoup(r.content)\n",
    "\n",
    "# Find all relevant places in the HTML code where names are stored\n",
    "speaker = soup.findAll(\"ul\", {\"class\" : \"nav_list\"})\n",
    "chair = soup.findAll(\"h2\")\n",
    "table = soup.find(\"table\", {\"class\" : \"tutorials\"})\n",
    "table = table.find_all(\"td\")\n",
    "main = soup_OPTIONAL1.find(\"section\", {\"id\" : \"main\"})\n",
    "names_members = main.findAll(\"li\")\n",
    "names_teachers = soup_OPTIONAL2.findAll(\"div\", {\"class\" : \"col-5 col-12-medium\"})\n",
    "\n",
    "# Loop through the HTML code and extract names\n",
    "keynote_names = [table[k].text.lower().split(\"- \")[1] for k in range(len(table)) if \"Keynote\" in table[k].text]\n",
    "chair_names = [chair[k].text.lower().split(\": \")[2] for k in range(len(chair)) if \"Chair\" in chair[k].text]\n",
    "speaker_names = [speaker[k].find_all(\"i\")[j].text.lower().split(\", \")  for k in range(len(speaker)) for j in range(len(speaker[k].find_all(\"i\")))]\n",
    "speaker_names = sum(speaker_names, [])\n",
    "names_members_lst = [names_members[i].find(\"b\").text.lower() for i in range(len(names_members))]\n",
    "names_teachers = [names_teachers[i].findAll(\"li\")[k].find(\"b\").text.lower() for i in range(len(names_teachers)) for k in range(len(names_teachers[i].findAll(\"li\")))]\n",
    "\n",
    "\n",
    "\n",
    "# Print results for each category\n",
    "print(f\"Number of unique speakers:  {len(set(speaker_names))}\")\n",
    "print(f\"Number of unique keynote speakers:  {len(set(keynote_names))}\")\n",
    "print(f\"Number of unique chairs:  {len(set(chair_names))}\")\n",
    "print(f\"Number of unique members from optional link1:  {len(set(names_members_lst))}\")\n",
    "print(f\"Number of unique teachers from optional link2:  {len(set(names_teachers))}\")\n",
    "\n",
    "# Add all names together to find total unique names\n",
    "total_names = speaker_names + keynote_names + chair_names + names_members_lst + names_teachers\n",
    "df = pd.DataFrame(total_names, columns = [\"Name\"])\n",
    "df[\"Name\"] = df[\"Name\"].str.replace(\".\", \"\")\n",
    "uniq_names = pd.DataFrame(set(df[\"Name\"]), columns=[\"Name\"])\n",
    "#uniq_names = uniq_names.sort_values('Name', ascending=True)\n",
    "print(f\"Total number of unique speakers:  {len((uniq_names))}\")\n",
    "\n",
    "pd.DataFrame(uniq_names).to_csv(\"data/authors_part1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**The process of the web-scraping:** \n",
    "\n",
    "In the process of web-scraping the website and collect the specific names of all the researchers, a thourough investigation of the HTML setup was initiated in order to understand the hierarchical and nested structure of the page. The main structure of the page was after inspection divided into 3 main parts, where different approaches were utilized in order to access the data from different structures:\n",
    "1. collect the names og the key-note speakers from the overview table structure\n",
    "2. collect the names for the chair speakers in the \"h2\" sections\n",
    "3. collect names from the text sections in the \"ul\" sections "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Ready Made vs Custom Made Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Custom-made data**, as used in Centola's experiment, offers the advantage of experimental control and manipulation, allowing the researchers to precisely design conditions and variables and data is not incomplete, since all the needed data is collected. However, it can be time-consuming and costly to collect, and it may lack real-world validity due to potential response biases when trying to record behaviors with digital systems that are highly engineered to induce specific behaviors. \n",
    "**Ready-made data**, like used in Nicolaides's study, offers readily available information as it is always-on, and then enables the study of unexpected events and real-time measurements. It reflects real-world behavior but may lack experimental control and relevance to specific research questions, as samples can be incomplete or biased. It also poses challenges in identifying and controlling confounding variables and can be dirty and difficult to access.\n",
    "\n",
    "2. To be made....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Gathering Research Articles using the OpenAlex API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the assignment API endpoints from the OpenAlex API are used to collect Research Articles from IC2S2 Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'https://api.openalex.org/'\n",
    "RESOURCE = 'works'\n",
    "COMPLETE_URL = BASE_URL + RESOURCE\n",
    "\n",
    "df = pd.read_csv('data/authors_part3.csv', index_col=[0])\n",
    "\n",
    "IC2S2_author = df.loc[df['works_count'] > 5]\n",
    "IC2S2_authors = IC2S2_author.loc[IC2S2_author['works_count'] < 5000]\n",
    "\n",
    "IC2S2_authors = IC2S2_authors.reset_index()\n",
    "\n",
    "lst = [str(IC2S2_authors['id'][i]) + '|' for i in range(len(IC2S2_authors['id']))]\n",
    "n = 25\n",
    "\n",
    "chunked_lists = [lst[i:i + n] for i in range(0, len(lst), n)]\n",
    "\n",
    "# Filters for the API\n",
    "Filter1 = \"cited_by_count:>10\"\n",
    "Filter2 = \"authors_count:<10\"\n",
    "\n",
    "all_concepts = requests.get(BASE_URL+\"concepts\", params={\"filter\":\"level:0\"}).json()\n",
    "\n",
    "social_concepts= [i[\"id\"] for i in all_concepts[\"results\"] if i['display_name'] in ([\"Sociology\",\"Psychology\",\"Economics\",\"Political Science\"])]\n",
    "math_concepts= [i[\"id\"] for i in all_concepts[\"results\"] if i['display_name'] in ([\"Mathematics\",\"Physics\",\"Computer Science\"])]\n",
    "\n",
    "Filter3=\"concepts.id:\"+\"|\".join(social_concepts)\n",
    "Filter4=\"concepts.id:\"+\"|\".join(math_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/42 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [01:04<00:00,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "IC2S2_papers = {}\n",
    "IC2S2_abstracts = {}\n",
    "\n",
    "def get_papers_and_abstracts(chunked_list):\n",
    "    ids = ''.join(chunked_list)\n",
    "    ids = ids[:-1]\n",
    "    cursorState = '*'\n",
    "    while cursorState:\n",
    "        PARAMS = {\"per_page\":200,\n",
    "                \"filter\" :f'{\",\".join([Filter1,Filter2,Filter3,Filter4])},authorships.author.id:{ids}',\n",
    "                \"cursor\": f'{cursorState}'}\n",
    "            \n",
    "        response = requests.get(COMPLETE_URL, params=PARAMS).json()\n",
    "        if response['meta']['next_cursor']:\n",
    "            cursorState = str(response['meta']['next_cursor'])\n",
    "        else: cursorState = None\n",
    "\n",
    "        for i in range(len(response['results'])):\n",
    "            id = response['results'][i]['id']\n",
    "            pub_year = response['results'][i]['publication_year']\n",
    "            cited = response['results'][i]['cited_by_count']\n",
    "            authors = [response['results'][i]['authorships'][l]['author']['id'] for l in range(len(response['results'][i]['authorships']))]\n",
    "            title = response['results'][i]['title']\n",
    "            abstract = response['results'][i]['abstract_inverted_index']\n",
    "            IC2S2_papers[str(id)] = {'id':id, 'publication_year':pub_year,'cited_by_count':cited, 'author_ids':authors}\n",
    "            IC2S2_abstracts[str(id)] = {'id':id, 'title':title, 'abstract_inverted_index':abstract}\n",
    "    \n",
    "    return IC2S2_papers, IC2S2_abstracts\n",
    "\n",
    "results = Parallel(n_jobs=2)(delayed(get_papers_and_abstracts)(chunked_list) for chunked_list in tqdm(chunked_lists))\n",
    "\n",
    "\n",
    "papers_dict = {k: v for result in results for k, v in result[0].items()}\n",
    "abstracts_dict = {k: v for result in results for k, v in result[1].items()}\n",
    "\n",
    "Papers = pd.DataFrame(papers_dict).transpose()\n",
    "Abstracts = pd.DataFrame(abstracts_dict).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'https://api.openalex.org/'\n",
    "RESOURCE = 'authors'\n",
    "COMPLETE_URL = BASE_URL + RESOURCE\n",
    "unique_authors = list(set([i for a in Papers['author_ids'] for i in a]))\n",
    "new_authors = list(set(unique_authors).difference(list(IC2S2_authors['id'])))\n",
    "\n",
    "lst = [str(new_authors[i]) + '|' for i in range(len(new_authors))]\n",
    "n = 25\n",
    "\n",
    "chunked_lists_new = [lst[i:i + n] for i in range(0, len(lst), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_author_data(chunk):\n",
    "    cursorState = '*'\n",
    "    ids = ''.join(chunk)\n",
    "    ids = ids[:-1]\n",
    "    responses = {}\n",
    "    count = 0\n",
    "    while cursorState:\n",
    "        PARAMS = {\"per_page\":200,\n",
    "                \"filter\" :f'ids.openalex:{ids}',\n",
    "                \"cursor\": f'{cursorState}'}\n",
    "        \n",
    "        responses[str(count)] = requests.get(COMPLETE_URL, params=PARAMS).json()\n",
    "        if responses[str(count)]['meta']['next_cursor']:\n",
    "            cursorState = str(responses[str(count)]['meta']['next_cursor'])\n",
    "        else: cursorState = None\n",
    "        count += 1\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_data(response):\n",
    "    lists = []\n",
    "    for idx in response:\n",
    "        result = response[str(idx)]['results']\n",
    "        CO_list = {'display_name':[],'id':[],'works_api_url':[],'h_index':[],'works_count':[],'country_code':[]}\n",
    "        for i in range(len(result)):\n",
    "            CO_list['display_name']+=[result[i]['display_name']]\n",
    "\n",
    "            CO_list['id']+=[result[i]['id']]\n",
    "            \n",
    "            CO_list['works_api_url']+=[result[i]['works_api_url']]\n",
    "            \n",
    "            CO_list['h_index']+=[result[i]['summary_stats']['h_index']]\n",
    "            CO_list['works_count']+=[result[i]['works_count']]\n",
    "            \n",
    "            if result[i]['last_known_institution'] is not None:\n",
    "                CO_list['country_code']+=[result[i]['last_known_institution']['country_code']]\n",
    "            else: \n",
    "                CO_list['country_code']+=['None']\n",
    "        if len(CO_list['display_name'])>= 1:\n",
    "            lists.append(CO_list)\n",
    "    \n",
    "    return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 311/311 [03:17<00:00,  1.58it/s]\n",
      "100%|██████████| 311/311 [00:00<00:00, 1525.02it/s]\n"
     ]
    }
   ],
   "source": [
    "A_data = Parallel(n_jobs=2)(delayed(fetch_author_data)(chunk) for chunk in tqdm(chunked_lists_new))\n",
    "author_list = Parallel(n_jobs=2)(delayed(query_data)(A_data[idx]) for idx in tqdm(range(len(A_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(author_list[0][0])\n",
    "for i in range(1,len(author_list)):\n",
    "    df_authors = pd.concat([df,pd.DataFrame(author_list[i][0])], ignore_index=True)\n",
    "    df = df_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'https://api.openalex.org/'\n",
    "RESOURCE = 'works'\n",
    "COMPLETE_URL = BASE_URL + RESOURCE\n",
    "\n",
    "def fetch_works_data(chunk):\n",
    "    ids = ''.join(chunk)\n",
    "    ids = ids[:-1]\n",
    "    cursorState = '*'\n",
    "    responses = {}\n",
    "    count = 0\n",
    "    while cursorState:\n",
    "        PARAMS = {\"per_page\":200,\n",
    "                \"filter\" :f'{\",\".join([Filter1,Filter2,Filter3,Filter4])},authorships.author.id:{ids}',\n",
    "                \"cursor\": f'{cursorState}'}\n",
    "            \n",
    "        responses[str(count)] = requests.get(COMPLETE_URL, params=PARAMS).json()\n",
    "        if responses[str(count)]['meta']['next_cursor']:\n",
    "            cursorState = str(responses[str(count)]['meta']['next_cursor'])\n",
    "        else: cursorState = None\n",
    "        count+=1\n",
    "    return responses\n",
    "\n",
    "def query_data_works_paper(response):\n",
    "    lists = []\n",
    "    for idx in response:\n",
    "        result = response[str(idx)]['results']\n",
    "        Paper_list = {'id':[],'publication_year':[],'cited_by_count':[],'author_ids':[]}\n",
    "        for i in range(len(result)):\n",
    "            Paper_list['id'] += [result[i]['id']]\n",
    "            Paper_list['publication_year'] += [result[i]['publication_year']]\n",
    "            Paper_list['cited_by_count'] += [result[i]['cited_by_count']]\n",
    "            \n",
    "            test = [result[i]['authorships'][l]['author']['id'] for l in range(len(result[i]['authorships']))]\n",
    "            Paper_list['author_ids'] += [list(set(test).intersection(unique_authors))]\n",
    "        if len(Paper_list['id'])>= 1:\n",
    "            lists.append(Paper_list)\n",
    "    return lists\n",
    "\n",
    "def query_data_works_abstract(response):\n",
    "    lists = []\n",
    "    for idx in response:\n",
    "        result = response[str(idx)]['results']\n",
    "        Paper_list = {'id':[],'title':[],'abstract_inverted_index':[]}\n",
    "        for i in range(len(result)):\n",
    "            Paper_list['id'] += [result[i]['id']]\n",
    "            Paper_list['title'] += [result[i]['title']]\n",
    "            Paper_list['abstract_inverted_index'] += [result[i]['abstract_inverted_index']]\n",
    "            \n",
    "        if len(Paper_list['id'])>= 1:\n",
    "            lists.append(Paper_list)\n",
    "    return lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 311/311 [12:57<00:00,  2.50s/it]\n",
      "100%|██████████| 311/311 [00:11<00:00, 27.52it/s]\n",
      "100%|██████████| 311/311 [02:01<00:00,  2.55it/s]\n"
     ]
    }
   ],
   "source": [
    "CO_works_data = Parallel(n_jobs=2)(delayed(fetch_works_data)(chunk) for chunk in tqdm(chunked_lists_new))\n",
    "Paper_data = Parallel(n_jobs=2)(delayed(query_data_works_paper)(CO_works_data[idx]) for idx in tqdm(range(len(CO_works_data))))\n",
    "Abstract_data = Parallel(n_jobs=2)(delayed(query_data_works_abstract)(CO_works_data[idx]) for idx in tqdm(range(len(CO_works_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Paper_data[0][0])\n",
    "for i in range(1,len(Paper_data)):\n",
    "    df_paper = pd.concat([df,pd.DataFrame(Paper_data[i][0])], ignore_index=True)\n",
    "    df = df_paper\n",
    "    \n",
    "df = pd.DataFrame(Abstract_data[0][0])\n",
    "for i in range(1,len(Abstract_data)):\n",
    "    df_abstract = pd.concat([df,pd.DataFrame(Abstract_data[i][0])], ignore_index=True)\n",
    "    df = df_abstract\n",
    "    \n",
    "Co_Paper = df_paper\n",
    "Co_abstract = df_abstract\n",
    "Co_authors = df_authors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers = pd.concat([Papers,Co_Paper],ignore_index=True)\n",
    "all_abstracts = pd.concat([Abstracts,Co_abstract],ignore_index=True)\n",
    "all_author = pd.concat([IC2S2_authors,Co_authors],ignore_index=True)\n",
    "\n",
    "all_papers.drop_duplicates(subset=\"id\", keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_author.to_csv('data/IC2S2_all_authors',index=False)\n",
    "all_papers.to_csv('data/IC2S2_all_papers',index=False)\n",
    "all_abstracts.to_csv('data/IC2S2_all_abstracts',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Overview and Reflection questions:\n",
    "\n",
    "##### **How many works are listed in your IC2S2 papers dataframe?**\n",
    "\n",
    "- 45105 works are listed in the IC2S2 papers dataframe. \n",
    "\n",
    "##### **How many unique researchers have co-authored these works?**\n",
    "\n",
    "- The number of unique researchers is 8808.\n",
    "\n",
    "##### **Efficiency in code**\n",
    "- Our approach to making our code more efficient was to consider two main things. We realize that the bottleneck in the code is the rate limit of the API which is 2 request per second. We therefore implemented, instead of using the 'search' parameter where one author can be requested at a time, we use in the filter parameter 'authorships.author.id' to request 25 authors at a time where we concatenate the author ids with the \"|\" (OR) parameter. This way we can request 25 authors at a time.\n",
    "Furthermore, we used the python library 'joblib' to parallelize the requests to the API. We changed all the for loops in our code, to function and run it through joblib to run more jobs at a time. We see that this has an impact in the run-time of the code, and make it more efficient.\n",
    "\n",
    "\n",
    "##### **Filtering Criteria and Dataset Relevance**\n",
    "- By setting our filter on the API request to set a threshold of works count between 5 and 5000, ensures that authors represented in the dataset. To further ensure that the dataset is relevant, we also set a filter to the citation count to be above 5 and that the amount of authors on the work is below 10. This ensures that the works and authors are relevant.\n",
    "Furthermore we filter for works that include both either \"Sociology\", \"Psychology\", \"Economics\", \"Political Science\" or \"Mathematics\",\"Physics\",\"Computer Science\". This leads to an overrepresentation in these field and will lead to an underrepresentation in fields that include \"History\", \"Biology\", \"Medicine\" and more fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Network of Computational Social Scientists\n",
    "\n",
    "In this part of the assignment we construct and investigate the Computational Social Scientists Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "all_papers = pd.read_csv(\"data/IC2S2_all_papers\")\n",
    "\n",
    "def list_maker(string):\n",
    "    return ast.literal_eval(string)\n",
    "\n",
    "all_papers = all_papers.drop_duplicates(['id'])\n",
    "Paper_author = all_papers['author_ids'].apply(list_maker)\n",
    "all_papers['author_ids'] = Paper_author\n",
    "\n",
    "#1.1 Weighted edgelist made from Papers dataset\n",
    "def find_pairs(my_list): # A function to rearange a list into a list of tuples of pairs\n",
    "    pairs = []\n",
    "    for n, author in enumerate(my_list):\n",
    "        for author2 in my_list[n+1:]:\n",
    "            pairs.append((author,author2))\n",
    "    return pairs\n",
    "\n",
    "pairs = all_papers['author_ids'].apply(lambda x: tuple(sorted(x))).apply(find_pairs) #rearange id's to avoid duplicates eg. (a,b),(b,a)\n",
    "all_pairs = pairs.explode()\n",
    "sorted_pairs = all_pairs.groupby(all_pairs).count().sort_values()\n",
    "edgelist = [(a1,a2,v) for (a1,a2), v in zip(sorted_pairs.index, sorted_pairs.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Graph construction based on the created edgelist\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(edgelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 save authors as nodes in the graph, with certain information about each, saved as attributes.\n",
    "Author_df = pd.read_csv('data/IC2S2_all_authors')\n",
    "Author_papers = all_papers.explode('author_ids')\n",
    "publication_df = Author_papers.groupby('author_ids')['publication_year'].min().reset_index()\n",
    "publication_df.columns = ['id', 'first_publication_year']\n",
    "cited_df = Author_papers.groupby('author_ids')['cited_by_count'].sum().reset_index()\n",
    "cited_df.columns = ['id','cited_by_count']\n",
    "Author_df = Author_df.merge(publication_df, on='id')\n",
    "Author_df = Author_df.merge(cited_df, on='id')\n",
    "Author_df = Author_df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in G.nodes:\n",
    "    if node in Author_df['id'].values:\n",
    "        G.nodes[node]['display_name'] = Author_df[Author_df['id']== node]['display_name'].values[0]\n",
    "        G.nodes[node]['country_code'] = Author_df[Author_df['id']== node]['country_code'].values[0]\n",
    "        G.nodes[node]['cited_by_count'] = Author_df[Author_df['id']== node]['cited_by_count'].values[0]\n",
    "        G.nodes[node]['first_publication_year'] = Author_df[Author_df['id']== node]['first_publication_year'].values[0]\n",
    "\n",
    "        #OBS gem som JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of nodes in G is 8464 and the amount of edges are 26290\n",
      "\n",
      "The denisty of G is 0.0007340414529877302\n",
      "\n",
      "Is the graph fully connected (not disconnected): False\n",
      "\n",
      "Since graph G is disconnected; there are 139 different connected components and 0 isolated nodes\n"
     ]
    }
   ],
   "source": [
    "#2.1 Network metrics\n",
    "CC = 0\n",
    "IN = 0\n",
    "components = nx.connected_components(G)\n",
    "for c in components:\n",
    "    CC += 1\n",
    "    if len(c) == 1:\n",
    "        IN += 1\n",
    "\n",
    "print(f'The number of nodes in G is {G.number_of_nodes()} and the amount of edges are {G.number_of_edges()}')\n",
    "print(f'\\nThe denisty of G is {nx.density(G)}')\n",
    "print(f'\\nIs the graph fully connected (not disconnected): {nx.is_connected(G)}')\n",
    "print(f'\\nSince graph G is disconnected; there are {CC} different connected components and {IN} isolated nodes')\n",
    "\n",
    "#OBS besvar tekst spørgsmålet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regarding the degree of nodes in G. \n",
      "average : 6.21219281663516\n",
      "median : 5\n",
      "mode 4\n",
      "min : 1\n",
      "max : 122 \n",
      "Regarding the weighted degree of nodes in G. \n",
      "average : 9.108931947069943\n",
      "median : 6\n",
      "mode 4\n",
      "min : 1\n",
      "max : 212 \n"
     ]
    }
   ],
   "source": [
    "s1 = sorted([val for idx, val in G.degree])\n",
    "s2 = sorted([val for idx, val in G.degree(weight='weight')])\n",
    "for n,sorted_degrees in enumerate([s1, s2]):\n",
    "    average_degree = sum(sorted_degrees)/G.number_of_nodes()\n",
    "    median_degree = sorted_degrees[int(len(sorted_degrees)/2)]\n",
    "    mode_degree = max(set(sorted_degrees),key=sorted_degrees.count)\n",
    "    min_degree = sorted_degrees[0]\n",
    "    max_degree = sorted_degrees[-1]\n",
    "    if n == 0:\n",
    "        print(f'Regarding the degree of nodes in G. \\naverage : {average_degree}\\nmedian : {median_degree}\\nmode {mode_degree}\\nmin : {min_degree}\\nmax : {max_degree} ')\n",
    "    else: print(f'Regarding the weighted degree of nodes in G. \\naverage : {average_degree}\\nmedian : {median_degree}\\nmode {mode_degree}\\nmin : {min_degree}\\nmax : {max_degree} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>degree</th>\n",
       "      <th>names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>https://openalex.org/A5088141761</td>\n",
       "      <td>122</td>\n",
       "      <td>Jonathan D. Cohen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2956</th>\n",
       "      <td>https://openalex.org/A5029100305</td>\n",
       "      <td>104</td>\n",
       "      <td>Denny Borsboom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>https://openalex.org/A5075080019</td>\n",
       "      <td>95</td>\n",
       "      <td>Qin Li</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>https://openalex.org/A5055710645</td>\n",
       "      <td>94</td>\n",
       "      <td>Jon Kleinberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>https://openalex.org/A5065243448</td>\n",
       "      <td>92</td>\n",
       "      <td>Qin Wang</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id  degree              names\n",
       "978   https://openalex.org/A5088141761     122  Jonathan D. Cohen\n",
       "2956  https://openalex.org/A5029100305     104     Denny Borsboom\n",
       "579   https://openalex.org/A5075080019      95             Qin Li\n",
       "925   https://openalex.org/A5055710645      94      Jon Kleinberg\n",
       "255   https://openalex.org/A5065243448      92           Qin Wang"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.3 Top 5 Authors, based on the amount of degrees (collaborative works with unique authors)\n",
    "top5 = pd.DataFrame(G.degree, columns=['id', 'degree']).sort_values(by='degree',ascending=False).head(5)\n",
    "top5['names'] = None\n",
    "display_names = [Author_df.loc[Author_df[\"id\"] == top5[\"id\"].values[i]][\"display_name\"].values[0] for i in range(len(top5))]\n",
    "top5['names'] = display_names\n",
    "top5\n",
    "\n",
    "#OBS BESVAR TEKST SPØRGSMÅL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comsocsci2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
